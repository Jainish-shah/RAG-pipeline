{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://dumps.wikimedia.org/enwiki/20241120/enwiki-20241120-pages-articles-multistream1.xml-p1p41242.bz2\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPSlnFEF2t4Q",
        "outputId": "66dce9bc-d726-4aac-bd02-4c309d9b95e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-03 07:47:01--  https://dumps.wikimedia.org/enwiki/20241120/enwiki-20241120-pages-articles-multistream1.xml-p1p41242.bz2\n",
            "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.71, 2620:0:861:3:208:80:154:71\n",
            "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.71|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 285038232 (272M) [application/octet-stream]\n",
            "Saving to: ‘enwiki-20241120-pages-articles-multistream1.xml-p1p41242.bz2’\n",
            "\n",
            "enwiki-20241120-pag 100%[===================>] 271.83M  4.28MB/s    in 64s     \n",
            "\n",
            "2024-12-03 07:48:05 (4.28 MB/s) - ‘enwiki-20241120-pages-articles-multistream1.xml-p1p41242.bz2’ saved [285038232/285038232]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets faiss-gpu\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmCdCscS224U",
        "outputId": "e61942f7-f1a0-4083-9eb3-715bbf6c69ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets faiss-cpu nltk requests\n",
        "!pip install git+https://github.com/attardi/wikiextractor.git  # For wikiextractor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7POgCBDf25ui",
        "outputId": "45f6f66b-1c7c-4bfa-f33e-cfa5d9ff88d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, faiss-cpu, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 faiss-cpu-1.9.0.post1 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting git+https://github.com/attardi/wikiextractor.git\n",
            "  Cloning https://github.com/attardi/wikiextractor.git to /tmp/pip-req-build-sltaz4rl\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/attardi/wikiextractor.git /tmp/pip-req-build-sltaz4rl\n",
            "  Resolved https://github.com/attardi/wikiextractor.git to commit 8f1b434a80608e1e313d38d263ed7c79c9ee75a9\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wikiextractor\n",
            "  Building wheel for wikiextractor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikiextractor: filename=wikiextractor-3.0.7-py3-none-any.whl size=47602 sha256=cf81207cd2a281dfb703e868bf721a08d3bafb88717d5b025eab8e27ffc9bf57\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kw7ltqvi/wheels/b3/fd/d7/2426213827cbddcdb8669bdcf34e9944c2ca46a59f1c3a41d3\n",
            "Successfully built wikiextractor\n",
            "Installing collected packages: wikiextractor\n",
            "Successfully installed wikiextractor-3.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m wikiextractor.WikiExtractor enwiki-20241120-pages-articles-multistream1.xml-p1p41242.bz2 --json --output extracted_wikipedia\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EizM5HzU29rg",
        "outputId": "8f4f1a86-ec30-4e07-cf4a-16f02b931a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Preprocessing 'enwiki-20241120-pages-articles-multistream1.xml-p1p41242.bz2' to collect template definitions: this may take some time.\n",
            "INFO: Loaded 0 templates in 42.9s\n",
            "INFO: Starting page extraction from enwiki-20241120-pages-articles-multistream1.xml-p1p41242.bz2.\n",
            "INFO: Using 7 extract processes.\n",
            "WARNING: Template errors in article 'List of country calling codes' (5770): title(1) recursion(0, 0, 0)\n",
            "WARNING: Template errors in article 'Koan' (16862): title(1) recursion(0, 0, 0)\n",
            "INFO: Finished 7-process extraction of 27140 articles in 57.0s (476.2 art/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Define the path to the extracted Wikipedia data\n",
        "base_path = 'extracted_wikipedia'\n",
        "\n",
        "# Create a list to hold all the documents\n",
        "wiki_documents = []\n",
        "\n",
        "# Process each subfolder and file within the base directory\n",
        "for subfolder in ['AA', 'AB', 'AC', 'AD']:\n",
        "    folder_path = os.path.join(base_path, subfolder)\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                entry = json.loads(line)\n",
        "                title = entry['title']\n",
        "                text = entry['text']\n",
        "                if text:  # Ensure there is text to process\n",
        "                    wiki_documents.append(Document(page_content=text, metadata={'title': title}))\n",
        "\n",
        "# Define a text splitter to handle large texts\n",
        "wiki_text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "\n",
        "# Split the documents if needed\n",
        "wiki_splits = wiki_text_splitter.split_documents(wiki_documents)\n",
        "\n",
        "# Example: Print the number of splits and a sample to check\n",
        "print(f\"Total number of document splits: {len(wiki_splits)}\")\n",
        "if wiki_splits:\n",
        "    print(f\"Sample split content: {wiki_splits[0].page_content[:500]}\")  # Print the first 500 characters of the first split\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn8ocE7z3BX2",
        "outputId": "2f8f14c3-e8d2-47a5-9371-d8b7bad3444c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of document splits: 1784814\n",
            "Sample split content: David Ricardo (18 April 1772 – 11 September 1823) was a British political economist, politician, and member of Parliament. He is recognized as one of the most influential classical economists, alongside figures such as Thomas Malthus, Adam Smith and James Mill.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain\n",
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"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\"\n",
        "os.environ['OPENAI_API_KEY'] = \"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\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YwFrETcM3ExE",
        "outputId": "ea1b360e-871e-4f21-d9eb-de80b6d973ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.10-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.21-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.11.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.9-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.21 (from langchain_community)\n",
            "  Downloading langchain_core-0.3.21-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.143)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.54.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.54.4)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (24.2)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.9.2)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.32.1-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n",
            "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.20.3)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.6)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.68.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.13.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.11)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.27.2)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.17.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.1.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain_community) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (0.7.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.28.2 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.28.2-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading protobuf-5.29.0-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-instrumentation==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b2)\n",
            "Collecting opentelemetry-util-http==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.23.4)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb) (0.26.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (2024.9.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading langchain_community-0.3.9-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.2.10-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
            "Downloading chromadb-0.5.21-py3-none-any.whl (628 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.9-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.21-py3-none-any.whl (409 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m120.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.28.2-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.28.2-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.28.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl (6.9 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Downloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.32.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading protobuf-5.29.0-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.6/442.6 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=8188ee003daac5cdc87a94f86344c164fd6edf90290c5291067af1926cbe1dca\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, websockets, uvloop, uvicorn, types-requests, python-dotenv, pyproject_hooks, protobuf, overrides, opentelemetry-util-http, mypy-extensions, mmh3, marshmallow, humanfriendly, httpx-sse, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, typing-inspect, tiktoken, starlette, posthog, opentelemetry-proto, langchainhub, coloredlogs, build, pydantic-settings, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, fastapi, dataclasses-json, opentelemetry-instrumentation, langchain-core, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain-openai, opentelemetry-instrumentation-fastapi, langchain, langchain_community, chromadb\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.19\n",
            "    Uninstalling langchain-core-0.3.19:\n",
            "      Successfully uninstalled langchain-core-0.3.19\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.7\n",
            "    Uninstalling langchain-0.3.7:\n",
            "      Successfully uninstalled langchain-0.3.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.0 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.21 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.9 fastapi-0.115.5 httptools-0.6.4 httpx-sse-0.4.0 humanfriendly-10.0 kubernetes-31.0.0 langchain-0.3.9 langchain-core-0.3.21 langchain-openai-0.2.10 langchain_community-0.3.9 langchainhub-0.1.21 marshmallow-3.23.1 mmh3-5.0.1 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.20.1 opentelemetry-exporter-otlp-proto-common-1.28.2 opentelemetry-exporter-otlp-proto-grpc-1.28.2 opentelemetry-instrumentation-0.49b2 opentelemetry-instrumentation-asgi-0.49b2 opentelemetry-instrumentation-fastapi-0.49b2 opentelemetry-proto-1.28.2 opentelemetry-util-http-0.49b2 overrides-7.7.0 posthog-3.7.4 protobuf-5.29.0 pydantic-settings-2.6.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.0.1 starlette-0.41.3 tiktoken-0.8.0 types-requests-2.32.0.20241016 typing-inspect-0.9.0 uvicorn-0.32.1 uvloop-0.21.0 watchfiles-1.0.0 websockets-14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Proceed with vectorstore creation\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n"
      ],
      "metadata": {
        "id": "b43aglTy5YSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Generate embeddings for the splits\n",
        "wiki_embeddings = OpenAIEmbeddings()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8SDU4rAt5bj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "wiki_vectorstore = FAISS.from_documents(wiki_splits[:1000], wiki_embeddings)\n",
        "\n",
        "# Use as a retriever\n",
        "wiki_retriever = wiki_vectorstore.as_retriever()\n"
      ],
      "metadata": {
        "id": "XJFYCeUx5cY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to retrieve and concatenate top-k document contents\n",
        "def retrieve_documents_chroma(query, retriever, top_k=10):\n",
        "    # Retrieve top-k results\n",
        "    results = retriever.get_relevant_documents(query)[:top_k]  # Limit results to top_k\n",
        "\n",
        "    # Concatenate the content of the results\n",
        "    concatenated_content = \" \".join([result.page_content for result in results])\n",
        "\n",
        "    return concatenated_content\n",
        "\n",
        "# Example query\n",
        "query = \"Who invented diesel engine?\"\n",
        "\n",
        "# Retrieve concatenated content\n",
        "top_docs_content = retrieve_documents_chroma(query, wiki_retriever)\n",
        "\n",
        "# Print the concatenated result\n",
        "print(f\"Concatenated content for query: '{query}'\\n\")\n",
        "print(top_docs_content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJn2Gs4z5eaK",
        "outputId": "c385fe40-1518-4f58-f87d-e84ff7151c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Concatenated content for query: 'Who invented diesel engine?'\n",
            "\n",
            "Firsts - The National Motor Museum Trust: The National Motor Museum Trust. Home; ... He patented his design in 1911. Various motoring magazine pictures show Prince Henry of Prussia in a car with simple up and ... Stoewer - The Ultimate American Classic Car Resource: ... forcing the State of Prussia for the ... which developed 15 bhp and was ... echoed in the agressively ugly Prince Henry-type bodies with their inswept side ... Car Resource: ... forcing the State of Prussia to intervene to ... The Stoewer Superior The D9 was developed ... in reviving car production in the Stoewer works and so ... NAG - The Ultimate American Classic Car Resource: The Story of NAG - Car makers for the ... which developed 15 bhp and was ... Henry of Prussia. 1 ... Early Benz Racing Cars at Rétromobile – Green and White ...: Early Benz Racing Cars at Rétromobile ... Automobile enthusiast Prince Henry of Prussia, ... was a 120 h.p. Grand Prix racing car of 1908. Newly developed for ... Motoring Firsts - The National Motor Museum Trust:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DTU_--N9rXuJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNSXxp_Zc8GL",
        "outputId": "f2b494c2-27ef-448f-9f3b-94de64dfc081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-03 08:02:07--  https://nlp.cs.washington.edu/triviaqa/data/triviaqa-unfiltered.tar.gz\n",
            "Resolving nlp.cs.washington.edu (nlp.cs.washington.edu)... 128.208.3.117, 2607:4000:200:12:3eec:efff:fe5e:6f68\n",
            "Connecting to nlp.cs.washington.edu (nlp.cs.washington.edu)|128.208.3.117|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 632549060 (603M) [application/x-gzip]\n",
            "Saving to: ‘triviaqa-unfiltered.tar.gz’\n",
            "\n",
            "triviaqa-unfiltered 100%[===================>] 603.25M  61.9MB/s    in 9.9s    \n",
            "\n",
            "2024-12-03 08:02:17 (60.9 MB/s) - ‘triviaqa-unfiltered.tar.gz’ saved [632549060/632549060]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://nlp.cs.washington.edu/triviaqa/data/triviaqa-unfiltered.tar.gz\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf triviaqa-unfiltered.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuEuibKNc_I9",
        "outputId": "85951683-f244-4f48-e3c9-919f6b8673bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "triviaqa-unfiltered/\n",
            "triviaqa-unfiltered/unfiltered-web-train.json\n",
            "triviaqa-unfiltered/README\n",
            "triviaqa-unfiltered/unfiltered-web-dev.json\n",
            "triviaqa-unfiltered/unfiltered-web-test-without-answers.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load JSON file\n",
        "with open(\"triviaqa-unfiltered/unfiltered-web-train.json\", \"r\") as f:\n",
        "    trivia_data = json.load(f).get(\"Data\", [])\n",
        "\n",
        "# Print to check if data is loaded\n",
        "print(\"Number of entries loaded:\", len(trivia_data))\n"
      ],
      "metadata": {
        "id": "y7RAiuuedBAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51a5c6b4-c965-4e1f-cc4b-decd2f1b0d39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries loaded: 87622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = []\n",
        "for entry in trivia_data:\n",
        "\n",
        "    if entry:\n",
        "        # Extract question and question ID\n",
        "        question = entry.get(\"Question\", \"\")\n",
        "\n",
        "        # Extract answer if it exists\n",
        "        answer_data = entry.get(\"Answer\", {})\n",
        "        answer = answer_data.get(\"Value\", \"\") if answer_data else None\n",
        "\n",
        "        # Construct context from EntityPages and SearchResults\n",
        "        context_parts = []\n",
        "\n",
        "        # Add titles and descriptions from EntityPages\n",
        "        for page in entry.get(\"EntityPages\", []):\n",
        "            title = page.get(\"Title\", \"\")\n",
        "            doc_source = page.get(\"DocSource\", \"\")\n",
        "            context_parts.append(f\"{title} (Source: {doc_source})\")\n",
        "\n",
        "        # Add descriptions from top-ranked SearchResults\n",
        "        for result in entry.get(\"SearchResults\", [])[:5]:  # Using the top 5 search results\n",
        "            title = result.get(\"Title\", \"\")\n",
        "            description = result.get(\"Description\", \"\")\n",
        "            if description:\n",
        "                context_parts.append(f\"{title}: {description}\")\n",
        "\n",
        "        # Combine all parts to form a detailed context\n",
        "        context = \" \".join(context_parts).strip()\n",
        "\n",
        "        # Append only if we have a question and non-empty context\n",
        "        if question and context:\n",
        "            docs.append({\n",
        "                \"question\": question,\n",
        "                \"context\": context,\n",
        "                \"answer\": answer\n",
        "            })\n",
        "\n",
        "# Check the results\n",
        "print(\"Number of documents extracted:\", len(docs))\n",
        "print(\"Sample document:\", docs[0] if docs else \"No documents extracted.\")\n",
        "\n",
        "\n",
        "docs_context = []\n",
        "for entry in trivia_data:\n",
        "\n",
        "    if entry:\n",
        "        # Extract question and question ID\n",
        "        question = entry.get(\"Question\", \"\")\n",
        "\n",
        "        # Extract answer if it exists\n",
        "        answer_data = entry.get(\"Answer\", {})\n",
        "        answer = answer_data.get(\"Value\", \"\") if answer_data else None\n",
        "\n",
        "        # Construct context from EntityPages and SearchResults\n",
        "        context_parts = []\n",
        "\n",
        "        # Add titles and descriptions from EntityPages\n",
        "        for page in entry.get(\"EntityPages\", []):\n",
        "            title = page.get(\"Title\", \"\")\n",
        "            doc_source = page.get(\"DocSource\", \"\")\n",
        "            context_parts.append(f\"{title} (Source: {doc_source})\")\n",
        "\n",
        "        # Add descriptions from top-ranked SearchResults\n",
        "        for result in entry.get(\"SearchResults\", [])[:5]:  # Using the top 5 search results\n",
        "            title = result.get(\"Title\", \"\")\n",
        "            description = result.get(\"Description\", \"\")\n",
        "            if description:\n",
        "                context_parts.append(f\"{title}: {description}\")\n",
        "\n",
        "        # Combine all parts to form a detailed context\n",
        "        context = \" \".join(context_parts).strip()\n",
        "\n",
        "        # Append only if we have a question and non-empty context\n",
        "        if question and context:\n",
        "            docs_context.append({\n",
        "                \"context\": context\n",
        "            })\n",
        "\n",
        "# Check the results\n",
        "print(\"Number of documents extracted:\", len(docs))\n",
        "print(\"Sample document:\", docs[0] if docs else \"No documents extracted.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mOlBMDVOdCop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1798bc0d-e6c5-4fcc-abfc-e8190a627e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents extracted: 87595\n",
            "Sample document: {'question': 'Who was President when the first Peanuts cartoon was published?', 'context': 'Peanuts (Source: TagMe) Peanuts Comic Strip, October 02, 1950 on GoComics.com: Welcome to GoComics.com, the world\\'s largest comic strip site for online ... If ever there is an iconic comic strip, it is Peanuts. ... The very first Peanuts cartoon! Peanuts First Published, 1950 | History/Herstory: Peanuts First Published, 1950. Posted by History/Herstory. 3. ... Sixty-three years ago on this day, the first Peanuts comic strip was published. On This Day: “Peanuts” Comic Strip Debuts - Web Resources: ... Charles Schulz’s “Peanuts” comic strip first appeared in several ... The first “Peanuts” cartoon, published Oct ... On This Day: “Peanuts ... Peanuts - Peanuts Wiki - Wikia: Peanuts 1950s. The first Peanuts comic strip from ... The final original daily Peanuts comic strip was published on ... Snoopy\\'s Christmas\" and \"Snoopy for President\". Obama Pays Tribute to Charlie Brown, Snoopy and the ...: ... volume of “The Complete Peanuts,” a series published over ... of “The Complete Peanuts,” for which President Obama ... first “Peanuts ...', 'answer': 'Harry Truman'}\n",
            "Number of documents extracted: 87595\n",
            "Sample document: {'question': 'Who was President when the first Peanuts cartoon was published?', 'context': 'Peanuts (Source: TagMe) Peanuts Comic Strip, October 02, 1950 on GoComics.com: Welcome to GoComics.com, the world\\'s largest comic strip site for online ... If ever there is an iconic comic strip, it is Peanuts. ... The very first Peanuts cartoon! Peanuts First Published, 1950 | History/Herstory: Peanuts First Published, 1950. Posted by History/Herstory. 3. ... Sixty-three years ago on this day, the first Peanuts comic strip was published. On This Day: “Peanuts” Comic Strip Debuts - Web Resources: ... Charles Schulz’s “Peanuts” comic strip first appeared in several ... The first “Peanuts” cartoon, published Oct ... On This Day: “Peanuts ... Peanuts - Peanuts Wiki - Wikia: Peanuts 1950s. The first Peanuts comic strip from ... The final original daily Peanuts comic strip was published on ... Snoopy\\'s Christmas\" and \"Snoopy for President\". Obama Pays Tribute to Charlie Brown, Snoopy and the ...: ... volume of “The Complete Peanuts,” a series published over ... of “The Complete Peanuts,” for which President Obama ... first “Peanuts ...', 'answer': 'Harry Truman'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the results\n",
        "print(\"Number of documents extracted:\", len(docs_context))\n",
        "print(\"Sample document:\", docs[0] if docs else \"No documents extracted.\")"
      ],
      "metadata": {
        "id": "iFmNokFEdTFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10ded2d2-3045-4960-a6d5-faa505cec7bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents extracted: 87595\n",
            "Sample document: {'question': 'Who was President when the first Peanuts cartoon was published?', 'context': 'Peanuts (Source: TagMe) Peanuts Comic Strip, October 02, 1950 on GoComics.com: Welcome to GoComics.com, the world\\'s largest comic strip site for online ... If ever there is an iconic comic strip, it is Peanuts. ... The very first Peanuts cartoon! Peanuts First Published, 1950 | History/Herstory: Peanuts First Published, 1950. Posted by History/Herstory. 3. ... Sixty-three years ago on this day, the first Peanuts comic strip was published. On This Day: “Peanuts” Comic Strip Debuts - Web Resources: ... Charles Schulz’s “Peanuts” comic strip first appeared in several ... The first “Peanuts” cartoon, published Oct ... On This Day: “Peanuts ... Peanuts - Peanuts Wiki - Wikia: Peanuts 1950s. The first Peanuts comic strip from ... The final original daily Peanuts comic strip was published on ... Snoopy\\'s Christmas\" and \"Snoopy for President\". Obama Pays Tribute to Charlie Brown, Snoopy and the ...: ... volume of “The Complete Peanuts,” a series published over ... of “The Complete Peanuts,” for which President Obama ... first “Peanuts ...', 'answer': 'Harry Truman'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain\n",
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"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\"\n",
        "os.environ['OPENAI_API_KEY'] = \"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\""
      ],
      "metadata": {
        "id": "5MQ0aA_BhG0F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "1df8b1df-d77c-4849-dd7a-c5678be62cf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.9)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.2.10)\n",
            "Requirement already satisfied: langchainhub in /usr/local/lib/python3.10/dist-packages (0.1.21)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.5.21)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.9)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.11.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.21 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.21)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.143)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.54.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.54.4)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (24.2)\n",
            "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.32.0.20241016)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.9.2)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.115.5)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.32.1)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.7.4)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.20.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.49b2)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n",
            "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.20.3)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.6)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.68.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.2.1)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.13.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (31.0.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (5.0.1)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.11)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.27.2)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.17.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.41.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.21->langchain_community) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain-openai) (0.7.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.2)\n",
            "Requirement already satisfied: opentelemetry-proto==1.28.2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.2)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b2)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b2)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b2)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.49b2 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.49b2->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.23.4)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb) (0.26.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (2024.9.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.21->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Convert each dictionary in docs to a Document object\n",
        "documents = [\n",
        "    Document(page_content=doc[\"context\"], metadata={\"context\": doc[\"context\"]})\n",
        "    for doc in docs\n",
        "]\n",
        "\n",
        "# Now use RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "splits = text_splitter.split_documents(documents)\n",
        "\n",
        "# Check the results\n",
        "print(\"Number of splits:\", len(splits))\n",
        "print(\"Sample split:\", splits[0] if splits else \"No splits generated.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g-o6JQAVhOHs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f5ef6e-ccaf-4944-8f39-9b412e7ff5c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of splits: 417701\n",
            "Sample split: page_content='Peanuts (Source: TagMe) Peanuts Comic Strip, October 02, 1950 on GoComics.com: Welcome to GoComics.com, the world's largest comic strip site for online ... If ever there is an iconic comic strip, it is Peanuts. ... The very first Peanuts cartoon! Peanuts First Published, 1950 | History/Herstory:' metadata={'context': 'Peanuts (Source: TagMe) Peanuts Comic Strip, October 02, 1950 on GoComics.com: Welcome to GoComics.com, the world\\'s largest comic strip site for online ... If ever there is an iconic comic strip, it is Peanuts. ... The very first Peanuts cartoon! Peanuts First Published, 1950 | History/Herstory: Peanuts First Published, 1950. Posted by History/Herstory. 3. ... Sixty-three years ago on this day, the first Peanuts comic strip was published. On This Day: “Peanuts” Comic Strip Debuts - Web Resources: ... Charles Schulz’s “Peanuts” comic strip first appeared in several ... The first “Peanuts” cartoon, published Oct ... On This Day: “Peanuts ... Peanuts - Peanuts Wiki - Wikia: Peanuts 1950s. The first Peanuts comic strip from ... The final original daily Peanuts comic strip was published on ... Snoopy\\'s Christmas\" and \"Snoopy for President\". Obama Pays Tribute to Charlie Brown, Snoopy and the ...: ... volume of “The Complete Peanuts,” a series published over ... of “The Complete Peanuts,” for which President Obama ... first “Peanuts ...'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Proceed with vectorstore creation\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Generate embeddings for the splits\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Index the splits into Chroma\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits[:100],\n",
        "    embedding=embeddings\n",
        ")\n",
        "\n",
        "# Use as a retriever\n",
        "retriever = vectorstore.as_retriever()\n"
      ],
      "metadata": {
        "id": "nRhnrOPYhrjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Template for multi-perspective queries\n",
        "template = \"\"\"Generate two alternative versions of this question to help in retrieving relevant documents:\n",
        "Original question: {question}\"\"\"\n",
        "\n",
        "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
        "generate_queries = (\n",
        "    prompt_perspectives\n",
        "    | ChatOpenAI(temperature=0)\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")\n"
      ],
      "metadata": {
        "id": "OAVXs2L8h7WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "eXDVc7iUnmwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.load import dumps, loads\n",
        "from operator import itemgetter\n",
        "\n",
        "# Multi Query: Generate Different Perspectives\n",
        "template = \"\"\"You are an AI language model assistant. Your task is to generate two\n",
        "different versions of the given user question to retrieve relevant documents from a vector\n",
        "database. By generating multiple perspectives on the user question, your goal is to help\n",
        "the user overcome some of the limitations of the distance-based similarity search.\n",
        "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
        "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_perspectives\n",
        "    | ChatOpenAI(temperature=0, model=\"gpt-4o\")\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")\n",
        "\n",
        "# Function to get unique documents\n",
        "def get_unique_union(documents: list[list]):\n",
        "    \"\"\" Unique union of retrieved docs \"\"\"\n",
        "    # Flatten list of lists, and convert each Document to string\n",
        "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
        "    # Get unique documents\n",
        "    unique_docs = list(set(flattened_docs))\n",
        "    # Return\n",
        "    return [loads(doc) for doc in unique_docs]\n",
        "\n",
        "# Initialize Chroma vectorstore for primary retrieval\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits[:100],  # First 100 documents for primary retrieval\n",
        "    embedding=embeddings\n",
        ")\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Initialize FAISS vectorstore for wiki-based retrieval\n",
        "wiki_vectorstore = FAISS.from_documents(\n",
        "    documents=splits[:1000],  # Wiki documents for fallback\n",
        "    embedding=embeddings\n",
        ")\n",
        "wiki_retriever = wiki_vectorstore.as_retriever()\n",
        "\n",
        "# Retrieval with fallback logic\n",
        "def retrieval_with_fallback(question):\n",
        "    # Primary retrieval with Chroma\n",
        "    retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
        "    primary_docs = retrieval_chain.invoke({\"question\": question})\n",
        "\n",
        "    if len(primary_docs) > 0:\n",
        "        return primary_docs\n",
        "\n",
        "    # Fallback to wiki retrieval with FAISS\n",
        "    wiki_docs = generate_queries | wiki_retriever.map() | get_unique_union\n",
        "    wiki_results = wiki_docs.invoke({\"question\": question})\n",
        "\n",
        "    if len(wiki_results) > 0:\n",
        "        return wiki_results\n",
        "\n",
        "    # No context available\n",
        "    return []\n",
        "\n",
        "# RAG Chain to answer questions based on retrieved context\n",
        "answer_template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(answer_template)\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "final_rag_chain = (\n",
        "    {\"context\": retrieval_with_fallback,\n",
        "     \"question\": itemgetter(\"question\")}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Example Usage\n",
        "question = \"who invented diesel engine?\"\n",
        "retrieved_docs = retrieval_with_fallback(question)\n",
        "\n",
        "if len(retrieved_docs) == 0:\n",
        "    print(\"No context present.\")\n",
        "else:\n",
        "    answer = final_rag_chain.invoke({\"question\": question, \"context\": retrieved_docs})\n",
        "    print(answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2WWk6-1nrB1",
        "outputId": "d3cf46a5-0a87-4838-a83c-ec495035859e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The diesel engine was invented by Rudolf Diesel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the TriviaQA dataset\n",
        "print(\"Loading TriviaQA dataset...\")\n",
        "with open(\"triviaqa-unfiltered/unfiltered-web-train.json\", \"r\") as f:\n",
        "    trivia_data = json.load(f).get(\"Data\", [])\n",
        "\n",
        "print(f\"Total entries in dataset: {len(trivia_data)}\")\n",
        "\n",
        "# Extract relevant information for evaluation\n",
        "print(\"Extracting questions and expected answers...\")\n",
        "evaluation_data = []\n",
        "for entry in trivia_data[:10]:  # Limit to the first 100 entries for efficiency\n",
        "    question = entry.get(\"Question\", \"\")\n",
        "    answer_data = entry.get(\"Answer\", {})\n",
        "    expected_answer = answer_data.get(\"Value\", \"\") if answer_data else None\n",
        "    if question and expected_answer:\n",
        "        evaluation_data.append({\n",
        "            \"question\": question,\n",
        "            \"expected_answer\": expected_answer\n",
        "        })\n",
        "\n",
        "print(f\"Total questions extracted for evaluation: {len(evaluation_data)}\")\n",
        "\n",
        "# Perform the evaluation\n",
        "def evaluate_triviaqa(data, verbose=False):\n",
        "    results = []\n",
        "    for i, item in enumerate(data):\n",
        "        question = item[\"question\"]\n",
        "        expected_answer = item[\"expected_answer\"]\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nEvaluating Question {i + 1}/{len(data)}\")\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"Expected Answer: {expected_answer}\")\n",
        "\n",
        "        # Generate alternative questions\n",
        "        if verbose:\n",
        "            print(\"Generating alternative questions...\")\n",
        "        try:\n",
        "            generated_alternatives = generate_queries.invoke({\"question\": question})\n",
        "        except TypeError as e:\n",
        "            print(f\"Error generating alternatives: {e}\")\n",
        "            generated_alternatives = []\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Generated Alternatives: {generated_alternatives}\")\n",
        "\n",
        "        # Retrieve relevant context\n",
        "        if verbose:\n",
        "            print(\"Retrieving relevant contexts...\")\n",
        "        try:\n",
        "            retrieved_contexts = retriever.invoke({\"question\": question})\n",
        "        except TypeError as e:\n",
        "            print(f\"Error retrieving contexts: {e}\")\n",
        "            retrieved_contexts = []\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Retrieved {len(retrieved_contexts)} context documents.\")\n",
        "\n",
        "        # Get the final answer\n",
        "        if verbose:\n",
        "            print(\"Generating the final answer using the RAG chain...\")\n",
        "        try:\n",
        "            final_answer = final_rag_chain.invoke({\"question\": question})\n",
        "        except TypeError as e:\n",
        "            print(f\"Error generating final answer: {e}\")\n",
        "            final_answer = \"\"\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Final Answer: {final_answer}\")\n",
        "\n",
        "        # Ensure inputs are strings for evaluation\n",
        "        question_str = str(question)\n",
        "        final_answer_str = str(final_answer)\n",
        "        expected_answer_str = str(expected_answer)\n",
        "\n",
        "        # Evaluate metrics\n",
        "        accuracy = 1 if expected_answer_str.lower() in final_answer_str.lower() else 0\n",
        "        relevance = 1 if any(\n",
        "            expected_answer_str.lower() in str(ctx[\"page_content\"]).lower() for ctx in retrieved_contexts\n",
        "        ) else 0\n",
        "        fluency = 1 if final_answer_str.strip() else 0  # Assuming fluent answers are non-empty\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Accuracy: {accuracy}\")\n",
        "            print(f\"Relevance: {relevance}\")\n",
        "            print(f\"Fluency: {fluency}\")\n",
        "\n",
        "        results.append({\n",
        "            \"question\": question_str,\n",
        "            \"expected_answer\": expected_answer_str,\n",
        "            \"generated_alternatives\": generated_alternatives,\n",
        "            \"final_answer\": final_answer_str,\n",
        "            \"accuracy\": accuracy,\n",
        "            \"relevance\": relevance,\n",
        "            \"fluency\": fluency\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run the evaluation with verbose output\n",
        "print(\"Starting evaluation...\")\n",
        "results = evaluate_triviaqa(evaluation_data, verbose=True)\n",
        "\n",
        "# Save the results to a JSON file\n",
        "print(\"Saving evaluation results to 'triviaqa_evaluation_results.json'...\")\n",
        "with open(\"triviaqa_evaluation_results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "# Print a summary of the evaluation\n",
        "total = len(results)\n",
        "accuracy_score = sum(r[\"accuracy\"] for r in results) / total\n",
        "relevance_score = sum(r[\"relevance\"] for r in results) / total\n",
        "fluency_score = sum(r[\"fluency\"] for r in results) / total\n",
        "\n",
        "print(f\"\\nEvaluation Summary:\")\n",
        "print(f\"Total Questions Evaluated: {total}\")\n",
        "print(f\"Accuracy: {accuracy_score * 100:.2f}%\")\n",
        "#print(f\"Relevance: {relevance_score * 100:.2f}%\")\n",
        "print(f\"Fluency: {fluency_score * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgoB0nM4ssZa",
        "outputId": "5fa43df4-81f5-44cb-f25c-05e3fff13bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading TriviaQA dataset...\n",
            "Total entries in dataset: 87622\n",
            "Extracting questions and expected answers...\n",
            "Total questions extracted for evaluation: 10\n",
            "Starting evaluation...\n",
            "\n",
            "Evaluating Question 1/10\n",
            "Question: Who was President when the first Peanuts cartoon was published?\n",
            "Expected Answer: Harry Truman\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['Which U.S. President was in office when the first Peanuts comic strip appeared?  ', 'During whose presidency was the inaugural Peanuts cartoon released?']\n",
            "Retrieving relevant contexts...\n",
            "Error retrieving contexts: argument 'text': 'dict' object cannot be converted to 'PyString'\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: President Obama was in office when the first Peanuts cartoon was published.\n",
            "Accuracy: 0\n",
            "Relevance: 0\n",
            "Fluency: 1\n",
            "\n",
            "Evaluating Question 2/10\n",
            "Question: Which American-born Sinclair won the Nobel Prize for Literature in 1930?\n",
            "Expected Answer: Sinclair Lewis\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['Who is the American author named Sinclair who received the Nobel Prize for Literature in 1930?  ', 'Can you tell me about the American writer Sinclair who was awarded the Nobel Prize for Literature in 1930?']\n",
            "Retrieving relevant contexts...\n",
            "Error retrieving contexts: argument 'text': 'dict' object cannot be converted to 'PyString'\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: Sinclair Lewis\n",
            "Accuracy: 1\n",
            "Relevance: 0\n",
            "Fluency: 1\n",
            "\n",
            "Evaluating Question 3/10\n",
            "Question: Where in England was Dame Judi Dench born?\n",
            "Expected Answer: York\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['What is the birthplace of Dame Judi Dench in England?  ', 'In which location in England did Dame Judi Dench come into the world?']\n",
            "Retrieving relevant contexts...\n",
            "Error retrieving contexts: argument 'text': 'dict' object cannot be converted to 'PyString'\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: Dame Judi Dench was born in York, North Yorkshire, England.\n",
            "Accuracy: 1\n",
            "Relevance: 0\n",
            "Fluency: 1\n",
            "\n",
            "Evaluating Question 4/10\n",
            "Question: William Christensen of Madison, New Jersey, has claimed to have the world's biggest collection of what?\n",
            "Expected Answer: Beer Cans\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['What unique collection does William Christensen from Madison, New Jersey, claim to have the largest of in the world?  ', \"In Madison, New Jersey, what does William Christensen assert to be the world's largest collection?\"]\n",
            "Retrieving relevant contexts...\n",
            "Error retrieving contexts: argument 'text': 'dict' object cannot be converted to 'PyString'\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: William Christensen of Madison, New Jersey, has claimed to have the world's biggest collection of apples.\n",
            "Accuracy: 0\n",
            "Relevance: 0\n",
            "Fluency: 1\n",
            "\n",
            "Evaluating Question 5/10\n",
            "Question: In which decade did Billboard magazine first publish and American hit chart?\n",
            "Expected Answer: 30s\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['When did Billboard magazine first introduce an American hit chart?  ', 'What was the first decade that Billboard magazine published an American music chart?']\n",
            "Retrieving relevant contexts...\n",
            "Error retrieving contexts: argument 'text': 'dict' object cannot be converted to 'PyString'\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: Billboard magazine first published an American hit chart in the 1940s.\n",
            "Accuracy: 0\n",
            "Relevance: 0\n",
            "Fluency: 1\n",
            "\n",
            "Evaluating Question 6/10\n",
            "Question: Where was horse racing's Breeders' Cup held in 1988?\n",
            "Expected Answer: Churchill Downs, Louisville, Kentucky\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: [\"In which location did the Breeders' Cup horse racing event take place in 1988?  \", \"What venue hosted the 1988 Breeders' Cup horse racing competition?\"]\n",
            "Retrieving relevant contexts...\n",
            "Error retrieving contexts: argument 'text': 'dict' object cannot be converted to 'PyString'\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: The Breeders' Cup was held at Woodbine in 1988.\n",
            "Accuracy: 0\n",
            "Relevance: 0\n",
            "Fluency: 1\n",
            "\n",
            "Evaluating Question 7/10\n",
            "Question: From which country did Angola achieve independence in 1975?\n",
            "Expected Answer: Portugal\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['Which nation did Angola gain independence from in the year 1975?  ', 'In 1975, Angola became independent from which colonial power?']\n",
            "Retrieving relevant contexts...\n",
            "Error retrieving contexts: argument 'text': 'dict' object cannot be converted to 'PyString'\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: Angola achieved independence from Portugal in 1975.\n",
            "Accuracy: 1\n",
            "Relevance: 0\n",
            "Fluency: 1\n",
            "\n",
            "Evaluating Question 8/10\n",
            "Question: Which city does David Soul come from?\n",
            "Expected Answer: Chicago\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['What is the birthplace of David Soul?  ', 'In which city was David Soul born?']\n",
            "Retrieving relevant contexts...\n",
            "Error retrieving contexts: argument 'text': 'dict' object cannot be converted to 'PyString'\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: David Soul comes from Kansas City.\n",
            "Accuracy: 0\n",
            "Relevance: 0\n",
            "Fluency: 1\n",
            "\n",
            "Evaluating Question 9/10\n",
            "Question: Who won Super Bowl XX?\n",
            "Expected Answer: Chicago Bears\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['Which team was victorious in Super Bowl XX?  ', 'Can you tell me the winner of the 20th Super Bowl?']\n",
            "Retrieving relevant contexts...\n",
            "Error retrieving contexts: argument 'text': 'dict' object cannot be converted to 'PyString'\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: The Chicago Bears won Super Bowl XX.\n",
            "Accuracy: 1\n",
            "Relevance: 0\n",
            "Fluency: 1\n",
            "\n",
            "Evaluating Question 10/10\n",
            "Question: Which was the first European country to abolish capital punishment?\n",
            "Expected Answer: Norway\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['What was the earliest European nation to eliminate the death penalty?  ', 'Which European country was the pioneer in ending capital punishment?']\n",
            "Retrieving relevant contexts...\n",
            "Error retrieving contexts: argument 'text': 'dict' object cannot be converted to 'PyString'\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: Portugal was the first European country to abolish capital punishment.\n",
            "Accuracy: 0\n",
            "Relevance: 0\n",
            "Fluency: 1\n",
            "Saving evaluation results to 'triviaqa_evaluation_results.json'...\n",
            "\n",
            "Evaluation Summary:\n",
            "Total Questions Evaluated: 10\n",
            "Accuracy: 40.00%\n",
            "Fluency: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the TriviaQA dataset\n",
        "print(\"Loading TriviaQA dataset...\")\n",
        "with open(\"triviaqa-unfiltered/unfiltered-web-train.json\", \"r\") as f:\n",
        "    trivia_data = json.load(f).get(\"Data\", [])\n",
        "\n",
        "print(f\"Total entries in dataset: {len(trivia_data)}\")\n",
        "\n",
        "# Extract relevant information for evaluation\n",
        "print(\"Extracting questions and expected answers...\")\n",
        "evaluation_data = []\n",
        "for entry in trivia_data[:10]:  # Limit to the first 100 entries for efficiency\n",
        "    question = entry.get(\"Question\", \"\")\n",
        "    answer_data = entry.get(\"Answer\", {})\n",
        "    expected_answer = answer_data.get(\"Value\", \"\") if answer_data else None\n",
        "    if question and expected_answer:\n",
        "        evaluation_data.append({\n",
        "            \"question\": question,\n",
        "            \"expected_answer\": expected_answer\n",
        "        })\n",
        "\n",
        "print(f\"Total questions extracted for evaluation: {len(evaluation_data)}\")\n",
        "\n",
        "# Define a function to compute RAGAs metrics\n",
        "def compute_ragas_metrics(expected, generated):\n",
        "    # Tokenize the expected and generated answers for comparison\n",
        "    expected_tokens = set(expected.lower().split())\n",
        "    generated_tokens = set(generated.lower().split())\n",
        "\n",
        "    # Calculate precision, recall, and F1\n",
        "    true_positives = len(expected_tokens & generated_tokens)\n",
        "    precision = true_positives / len(generated_tokens) if generated_tokens else 0\n",
        "    recall = true_positives / len(expected_tokens) if expected_tokens else 0\n",
        "    f1 = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "# Evaluate the dataset\n",
        "def evaluate_with_ragas(data, verbose=False):\n",
        "    results = []\n",
        "    for i, item in enumerate(data):\n",
        "        question = item[\"question\"]\n",
        "        expected_answer = item[\"expected_answer\"]\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nEvaluating Question {i + 1}/{len(data)}\")\n",
        "            print(f\"Question: {question}\")\n",
        "            print(f\"Expected Answer: {expected_answer}\")\n",
        "\n",
        "        # Generate alternative questions\n",
        "        try:\n",
        "            if verbose:\n",
        "                print(\"Generating alternative questions...\")\n",
        "            generated_alternatives = generate_queries.invoke({\"question\": question})\n",
        "        except TypeError as e:\n",
        "            print(f\"Error generating alternatives: {e}\")\n",
        "            generated_alternatives = []\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Generated Alternatives: {generated_alternatives}\")\n",
        "\n",
        "        # Retrieve relevant context\n",
        "        try:\n",
        "            if verbose:\n",
        "                print(\"Retrieving relevant contexts...\")\n",
        "            retrieved_contexts = retriever.invoke({\"question\": question})\n",
        "            # Ensure contexts are strings\n",
        "            retrieved_contexts = [\n",
        "                str(ctx[\"page_content\"]) for ctx in retrieved_contexts if \"page_content\" in ctx\n",
        "            ]\n",
        "        except TypeError as e:\n",
        "            #print(f\"Error retrieving contexts: {e}\")\n",
        "            retrieved_contexts = []\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Retrieved {len(retrieved_contexts)} context documents.\")\n",
        "\n",
        "        # Get the final answer\n",
        "        try:\n",
        "            if verbose:\n",
        "                print(\"Generating the final answer using the RAG chain...\")\n",
        "            final_answer = final_rag_chain.invoke({\"question\": question})\n",
        "            final_answer = str(final_answer)  # Ensure the answer is a string\n",
        "        except TypeError as e:\n",
        "            print(f\"Error generating final answer: {e}\")\n",
        "            final_answer = \"\"\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Final Answer: {final_answer}\")\n",
        "\n",
        "        # Compute RAGAs metrics\n",
        "        precision, recall, f1 = compute_ragas_metrics(expected_answer, final_answer)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Precision: {precision:.2f}\")\n",
        "            print(f\"Recall: {recall:.2f}\")\n",
        "            print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "        results.append({\n",
        "            \"question\": question,\n",
        "            \"expected_answer\": expected_answer,\n",
        "            \"final_answer\": final_answer,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1_score\": f1\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run the evaluation with verbose output\n",
        "print(\"Starting RAGAs evaluation...\")\n",
        "results = evaluate_with_ragas(evaluation_data, verbose=True)\n",
        "\n",
        "# Save the results to a JSON file\n",
        "print(\"Saving RAGAs evaluation results to 'ragas_evaluation_results.json'...\")\n",
        "with open(\"ragas_evaluation_results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "# Print a summary of the evaluation\n",
        "precision_scores = [r[\"precision\"] for r in results]\n",
        "recall_scores = [r[\"recall\"] for r in results]\n",
        "f1_scores = [r[\"f1_score\"] for r in results]\n",
        "\n",
        "average_precision = sum(precision_scores) / len(precision_scores)\n",
        "average_recall = sum(recall_scores) / len(recall_scores)\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "\n",
        "print(f\"\\nEvaluation Summary (RAGAs):\")\n",
        "print(f\"Average Precision: {average_precision * 100:.2f}%\")\n",
        "print(f\"Average Recall: {average_recall * 100:.2f}%\")\n",
        "print(f\"Average F1 Score: {average_f1 * 100:.2f}%\")\n",
        "\n",
        "# Run the evaluation with verbose output\n",
        "print(\"Starting RAGAs evaluation...\")\n",
        "results = evaluate_with_ragas(evaluation_data, verbose=True)\n",
        "\n",
        "# Save the results to a JSON file\n",
        "print(\"Saving RAGAs evaluation results to 'ragas_evaluation_results.json'...\")\n",
        "with open(\"ragas_evaluation_results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "\n",
        "# Print a summary of the evaluation\n",
        "precision_scores = [r[\"precision\"] for r in results]\n",
        "recall_scores = [r[\"recall\"] for r in results]\n",
        "f1_scores = [r[\"f1_score\"] for r in results]\n",
        "\n",
        "average_precision = sum(precision_scores) / len(precision_scores)\n",
        "average_recall = sum(recall_scores) / len(recall_scores)\n",
        "average_f1 = sum(f1_scores) / len(f1_scores)\n",
        "\n",
        "print(f\"\\nEvaluation Summary (RAGAs):\")\n",
        "print(f\"Average Precision: {average_precision * 100:.2f}%\")\n",
        "print(f\"Average Recall: {average_recall * 100:.2f}%\")\n",
        "print(f\"Average F1 Score: {average_f1 * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orZbCS09zOKq",
        "outputId": "e38b2708-8e77-4e3c-f173-e9adedf152e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading TriviaQA dataset...\n",
            "Total entries in dataset: 87622\n",
            "Extracting questions and expected answers...\n",
            "Total questions extracted for evaluation: 10\n",
            "Starting RAGAs evaluation...\n",
            "\n",
            "Evaluating Question 1/10\n",
            "Question: Who was President when the first Peanuts cartoon was published?\n",
            "Expected Answer: Harry Truman\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['Who was the U.S. President at the time the first Peanuts comic strip appeared in newspapers?  ', 'During whose presidency did the debut of the Peanuts cartoon take place?']\n",
            "Retrieving relevant contexts...\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: President Obama was in office when the first Peanuts cartoon was published.\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "\n",
            "Evaluating Question 2/10\n",
            "Question: Which American-born Sinclair won the Nobel Prize for Literature in 1930?\n",
            "Expected Answer: Sinclair Lewis\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['Who is the American author named Sinclair who received the Nobel Prize for Literature in 1930?  ', 'Can you tell me about the American writer Sinclair who was awarded the Nobel Prize for Literature in 1930?']\n",
            "Retrieving relevant contexts...\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: Sinclair Lewis\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1 Score: 1.00\n",
            "\n",
            "Evaluating Question 3/10\n",
            "Question: Where in England was Dame Judi Dench born?\n",
            "Expected Answer: York\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['What is the birthplace of Dame Judi Dench in England?  ', 'In which location in England was Dame Judi Dench born?']\n",
            "Retrieving relevant contexts...\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: Dame Judi Dench was born in York, North Yorkshire, England.\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "\n",
            "Evaluating Question 4/10\n",
            "Question: William Christensen of Madison, New Jersey, has claimed to have the world's biggest collection of what?\n",
            "Expected Answer: Beer Cans\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['What unique collection does William Christensen from Madison, New Jersey, claim to have the largest of in the world?  ', \"In Madison, New Jersey, what does William Christensen assert to be the world's largest collection?\"]\n",
            "Retrieving relevant contexts...\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: William Christensen of Madison, New Jersey, has claimed to have the world's biggest collection of apples.\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "\n",
            "Evaluating Question 5/10\n",
            "Question: In which decade did Billboard magazine first publish and American hit chart?\n",
            "Expected Answer: 30s\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['When did Billboard magazine first introduce an American hit chart?  ', 'What was the first decade that Billboard magazine published an American music chart?']\n",
            "Retrieving relevant contexts...\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: Billboard magazine first published an American hit chart in the 1940s.\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "\n",
            "Evaluating Question 6/10\n",
            "Question: Where was horse racing's Breeders' Cup held in 1988?\n",
            "Expected Answer: Churchill Downs, Louisville, Kentucky\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: [\"In which location did the Breeders' Cup horse racing event take place in 1988?  \", \"What venue hosted the 1988 Breeders' Cup horse racing event?\"]\n",
            "Retrieving relevant contexts...\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: The Breeders' Cup was held at Woodbine in 1988.\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "\n",
            "Evaluating Question 7/10\n",
            "Question: From which country did Angola achieve independence in 1975?\n",
            "Expected Answer: Portugal\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['Which nation did Angola gain independence from in the year 1975?  ', 'In 1975, Angola became independent from which colonial power?']\n",
            "Retrieving relevant contexts...\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: Angola achieved independence from Portugal in 1975.\n",
            "Precision: 0.14\n",
            "Recall: 1.00\n",
            "F1 Score: 0.25\n",
            "\n",
            "Evaluating Question 8/10\n",
            "Question: Which city does David Soul come from?\n",
            "Expected Answer: Chicago\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['What is the birthplace of David Soul?  ', 'In which city was David Soul born?']\n",
            "Retrieving relevant contexts...\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: David Soul comes from Kansas City.\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "\n",
            "Evaluating Question 9/10\n",
            "Question: Who won Super Bowl XX?\n",
            "Expected Answer: Chicago Bears\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['Which team was victorious in Super Bowl XX?  ', 'Can you tell me the winner of the 20th Super Bowl?']\n",
            "Retrieving relevant contexts...\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: The Chicago Bears won Super Bowl XX.\n",
            "Precision: 0.29\n",
            "Recall: 1.00\n",
            "F1 Score: 0.44\n",
            "\n",
            "Evaluating Question 10/10\n",
            "Question: Which was the first European country to abolish capital punishment?\n",
            "Expected Answer: Norway\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['What was the earliest European nation to eliminate the death penalty?  ', 'Which European country was the pioneer in ending capital punishment?']\n",
            "Retrieving relevant contexts...\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: Portugal was the first European country to abolish capital punishment.\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "Saving RAGAs evaluation results to 'ragas_evaluation_results.json'...\n",
            "\n",
            "Evaluation Summary (RAGAs):\n",
            "Average Precision: 14.29%\n",
            "Average Recall: 30.00%\n",
            "Average F1 Score: 16.94%\n",
            "Starting RAGAs evaluation...\n",
            "\n",
            "Evaluating Question 1/10\n",
            "Question: Who was President when the first Peanuts cartoon was published?\n",
            "Expected Answer: Harry Truman\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['Which U.S. President was in office when the first Peanuts comic strip appeared?  ', 'During whose presidency was the inaugural Peanuts cartoon released?']\n",
            "Retrieving relevant contexts...\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: President Obama was in office when the first Peanuts cartoon was published.\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "\n",
            "Evaluating Question 2/10\n",
            "Question: Which American-born Sinclair won the Nobel Prize for Literature in 1930?\n",
            "Expected Answer: Sinclair Lewis\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['Who is the American author named Sinclair who received the Nobel Prize for Literature in 1930?  ', 'Can you tell me about the American writer Sinclair who was awarded the Nobel Prize for Literature in 1930?']\n",
            "Retrieving relevant contexts...\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: Sinclair Lewis\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1 Score: 1.00\n",
            "\n",
            "Evaluating Question 3/10\n",
            "Question: Where in England was Dame Judi Dench born?\n",
            "Expected Answer: York\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['What is the birthplace of Dame Judi Dench in England?  ', 'In which location in England did Dame Judi Dench originate?']\n",
            "Retrieving relevant contexts...\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: Dame Judi Dench was born in York, North Yorkshire, England.\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "\n",
            "Evaluating Question 4/10\n",
            "Question: William Christensen of Madison, New Jersey, has claimed to have the world's biggest collection of what?\n",
            "Expected Answer: Beer Cans\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['What collection does William Christensen from Madison, New Jersey, claim to be the largest in the world?  ', \"What is the world's largest collection that William Christensen from Madison, New Jersey, claims to own?\"]\n",
            "Retrieving relevant contexts...\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: William Christensen of Madison, New Jersey, has claimed to have the world's biggest collection of apples.\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "\n",
            "Evaluating Question 5/10\n",
            "Question: In which decade did Billboard magazine first publish and American hit chart?\n",
            "Expected Answer: 30s\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['When did Billboard magazine first introduce an American hit chart?  ', 'What was the first decade that Billboard magazine published an American hit chart?']\n",
            "Retrieving relevant contexts...\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: Billboard magazine first published an American hit chart in the 1940s.\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "\n",
            "Evaluating Question 6/10\n",
            "Question: Where was horse racing's Breeders' Cup held in 1988?\n",
            "Expected Answer: Churchill Downs, Louisville, Kentucky\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: [\"In which location did the Breeders' Cup horse racing event take place in 1988?  \", \"What venue hosted the 1988 Breeders' Cup horse racing competition?\"]\n",
            "Retrieving relevant contexts...\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: The Breeders' Cup was held at Woodbine in 1988.\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "\n",
            "Evaluating Question 7/10\n",
            "Question: From which country did Angola achieve independence in 1975?\n",
            "Expected Answer: Portugal\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['Which nation did Angola gain independence from in the year 1975?  ', 'In 1975, Angola became independent from which colonial power?']\n",
            "Retrieving relevant contexts...\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: Angola achieved independence from Portugal in 1975.\n",
            "Precision: 0.14\n",
            "Recall: 1.00\n",
            "F1 Score: 0.25\n",
            "\n",
            "Evaluating Question 8/10\n",
            "Question: Which city does David Soul come from?\n",
            "Expected Answer: Chicago\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['What is the birthplace of David Soul?  ', 'In which city was David Soul born?']\n",
            "Retrieving relevant contexts...\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: David Soul comes from Kansas City.\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "\n",
            "Evaluating Question 9/10\n",
            "Question: Who won Super Bowl XX?\n",
            "Expected Answer: Chicago Bears\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['Which team was the champion of Super Bowl XX?  ', 'What was the outcome of Super Bowl XX in terms of the winning team?']\n",
            "Retrieving relevant contexts...\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: The Chicago Bears won Super Bowl XX.\n",
            "Precision: 0.29\n",
            "Recall: 1.00\n",
            "F1 Score: 0.44\n",
            "\n",
            "Evaluating Question 10/10\n",
            "Question: Which was the first European country to abolish capital punishment?\n",
            "Expected Answer: Norway\n",
            "Generating alternative questions...\n",
            "Generated Alternatives: ['What was the earliest European nation to eliminate the death penalty?  ', 'Which European country was the pioneer in ending capital punishment?']\n",
            "Retrieving relevant contexts...\n",
            "Retrieved 0 context documents.\n",
            "Generating the final answer using the RAG chain...\n",
            "Final Answer: Portugal was the first European country to abolish capital punishment.\n",
            "Precision: 0.00\n",
            "Recall: 0.00\n",
            "F1 Score: 0.00\n",
            "Saving RAGAs evaluation results to 'ragas_evaluation_results.json'...\n",
            "\n",
            "Evaluation Summary (RAGAs):\n",
            "Average Precision: 14.29%\n",
            "Average Recall: 30.00%\n",
            "Average F1 Score: 16.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eoNuY_VJ2whn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}